{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This Notebook Must Be Executed In A SageMaker Instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plagiarism Detection Model\n",
    "\n",
    "Now that the train and test sets have been processed and writen to `.csv` we can focus on modeling the data and deploying the model endpoint. \n",
    "\n",
    "We need to complete the following steps to finish the data science life-cycle for this project:\n",
    "\n",
    "* Upload data to S3.\n",
    "* Define a binary classification model and a training script.\n",
    "* Train the model and deploy it.\n",
    "* Evaluate deployed classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data from S3\n",
    "\n",
    "We already have `training.csv` and `test.csv` files with the features and class labels for the given corpus of plagiarized/non-plagiarized text data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# create an S3 bucket\n",
    "bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload your training data to S3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sagemaker-us-east-2-875690977746'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-2-875690977746/plagiarism_project/data/test.csv'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = 'plagiarism_project'\n",
    "prefix = 'data'\n",
    "\n",
    "# upload all data to S3\n",
    "sagemaker_session.upload_data(bucket=bucket, path='plagiarism_data/train.csv', key_prefix=data_dir+'/'+prefix)\n",
    "sagemaker_session.upload_data(bucket=bucket, path='plagiarism_data/test.csv', key_prefix=data_dir+'/'+prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check S3 Resources\n",
    "\n",
    "Test that the data has been successfully uploaded. The below cell prints out the items in your S3 bucket and will throw an error if it is empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm that data is in S3 bucket\n",
    "empty_check = []\n",
    "for obj in boto3.resource('s3').Bucket(bucket).objects.all():\n",
    "    empty_check.append(obj.key)\n",
    "\n",
    "assert len(empty_check) !=0, 'S3 bucket is empty.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Modeling\n",
    "\n",
    "Let's train to define and train a model!\n",
    "\n",
    "The data is loaded into S3 so that it is easy to reach. I will be using the Gradient Boosted classifier from `sklearn` that we tested previously in `3_Modeling_Trials.ipynb`\n",
    "\n",
    "## Training script \n",
    "\n",
    "To implement a custom classifier, we need to complete a `train.py` script. \n",
    "\n",
    "A typical training script includes:\n",
    "\n",
    "* Loads training data from a specified directory\n",
    "* Parses any training & model hyperparameters (ex. nodes in a neural network, training epochs, etc.)\n",
    "* Instantiates a model of your design, with any specified hyperparams\n",
    "* Trains that model \n",
    "* Finally, saves the model so that it can be hosted/deployed, later\n",
    "\n",
    "Since we have already tested a model and have one that we like, we will keep the framework in `train.py` but simply paste in the model parameters.\n",
    "\n",
    "### Defining and training a model\n",
    "We need a trainging file, `train.py` it has the following parts:\n",
    "\n",
    "1. Import any libraries needed\n",
    "2. Define additional model training hyperparameters using `parser.add_argument`\n",
    "2. Define a model in the `if __name__ == '__main__':` section\n",
    "3. Train the model in that same section\n",
    "\n",
    "I am using the below training script that I adapated from the Udacity course. Much of this file is pre-filled and is needed for running a custom PyTorch deep learning model. The SageMaker SKLearn model wrapper has it's own training script, but since we are uploading a model from a local training job, I believe this custom script is needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn.externals\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m joblib\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn.ensemble\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m GradientBoostingClassifier\n",
      "\n",
      "\u001b[37m# This is a general framework for testing models in SageMaker. \u001b[39;49;00m\n",
      "\u001b[37m# I'm keeping the structure the same, but using some shortcuts to make the process smoother.\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# Model load function\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\n",
      "    \u001b[33m\"\"\"Load model from the model_dir. This is the same model that is saved\u001b[39;49;00m\n",
      "\u001b[33m    in the main if statement.\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mLoading model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[37m# load using joblib\u001b[39;49;00m\n",
      "    model = joblib.load(os.path.join(model_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33mmodel.joblib\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mDone loading model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[34mreturn\u001b[39;49;00m model\n",
      "\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "    \n",
      "    \u001b[37m# All of the model parameters and training parameters are sent as arguments\u001b[39;49;00m\n",
      "    \n",
      "    \u001b[37m# Here we set up an argument parser to easily access the parameters\u001b[39;49;00m\n",
      "    parser = argparse.ArgumentParser()\n",
      "\n",
      "    \u001b[37m# SageMaker parameters, like the directories for training data and saving models are set automatically\u001b[39;49;00m\n",
      "    \n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--output-data-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_OUTPUT_DATA_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--data-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    \n",
      "\u001b[37m# Add any additional arguments that you will need to pass into your model\u001b[39;49;00m\n",
      "\u001b[37m# Since we already found parameters that we like, we can simply paste them in \u001b[39;49;00m\n",
      "\n",
      "\n",
      "    \u001b[37m# args holds all passed-in arguments\u001b[39;49;00m\n",
      "    args = parser.parse_args()\n",
      "\n",
      "    training_dir = args.data_dir\n",
      "    train_data = pd.read_csv(os.path.join(training_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33mtrain.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m), header=\u001b[36mNone\u001b[39;49;00m, names=\u001b[36mNone\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Labels are in the first column\u001b[39;49;00m\n",
      "    train_y = train_data.iloc[:,\u001b[34m0\u001b[39;49;00m]\n",
      "    train_x = train_data.iloc[:,\u001b[34m1\u001b[39;49;00m:]\n",
      "    \n",
      "    \n",
      "    model = GradientBoostingClassifier(criterion=\u001b[33m'\u001b[39;49;00m\u001b[33mfriedman_mse\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, init=\u001b[36mNone\u001b[39;49;00m,\n",
      "                           learning_rate=\u001b[34m0.020833333333333332\u001b[39;49;00m, loss=\u001b[33m'\u001b[39;49;00m\u001b[33mdeviance\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                           max_depth=\u001b[34m11\u001b[39;49;00m, max_features=\u001b[36mNone\u001b[39;49;00m, max_leaf_nodes=\u001b[36mNone\u001b[39;49;00m,\n",
      "                           min_impurity_decrease=\u001b[34m0.0\u001b[39;49;00m, min_impurity_split=\u001b[36mNone\u001b[39;49;00m,\n",
      "                           min_samples_leaf=\u001b[34m21\u001b[39;49;00m, min_samples_split=\u001b[34m16\u001b[39;49;00m,\n",
      "                           min_weight_fraction_leaf=\u001b[34m0.0\u001b[39;49;00m, n_estimators=\u001b[34m87\u001b[39;49;00m,\n",
      "                           n_iter_no_change=\u001b[36mNone\u001b[39;49;00m, presort=\u001b[33m'\u001b[39;49;00m\u001b[33mauto\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                           random_state=\u001b[34m945945\u001b[39;49;00m, subsample=\u001b[34m0.97\u001b[39;49;00m, tol=\u001b[34m0.0001\u001b[39;49;00m,\n",
      "                           validation_fraction=\u001b[34m0.1\u001b[39;49;00m, verbose=\u001b[34m0\u001b[39;49;00m,\n",
      "                           warm_start=\u001b[36mFalse\u001b[39;49;00m)\n",
      "    model.fit(X=train_x, y=train_y)\n",
      "    \n",
      "    \u001b[37m# Save the trained model\u001b[39;49;00m\n",
      "    joblib.dump(model, os.path.join(args.model_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33mmodel.joblib\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\n"
     ]
    }
   ],
   "source": [
    "!pygmentize source_sklearn/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Create an Estimator\n",
    "\n",
    "When a custom model is constructed in SageMaker, an entry point must be specified. This is the Python file which will be executed when the model is trained; the `train.py` function you specified above. To run a custom training script in SageMaker, construct an estimator, and fill in the appropriate constructor arguments:\n",
    "\n",
    "* **entry_point**: The path to the Python script SageMaker runs for training and prediction.\n",
    "* **source_dir**: The path to the training script directory.\n",
    "* **entry_point**: The path to the Python script SageMaker runs for training and prediction.\n",
    "* **source_dir**: The path to the training script directory.\n",
    "* **entry_point**: The path to the Python script SageMaker runs for training.\n",
    "* **source_dir**: The path to the training script directory.\n",
    "* **role**: Role ARN, which was specified, above.\n",
    "* **train_instance_count**: The number of training instances (should be left at 1).\n",
    "* **train_instance_type**: The type of SageMaker instance for training. \n",
    "* **sagemaker_session**: The session used to train on Sagemaker.\n",
    "* **hyperparameters** (optional): A dictionary `{'name':value, ..}` passed to the train function as hyperparameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-2-875690977746/plagiarism_project/data/output'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_output_path = \"s3://{}/{}/{}/output\".format(bucket, data_dir, prefix)\n",
    "s3_output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "# Define model role\n",
    "gbm_estimator = SKLearn(role=role,\n",
    "                        entry_point='train.py',\n",
    "                        source_dir='source_sklearn',\n",
    "                        train_instance_count=1,\n",
    "                        train_instance_type='ml.c4.xlarge',\n",
    "                        sagemaker_session=sagemaker_session,\n",
    "                        output_path=s3_output_path\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the estimator\n",
    "\n",
    "Create a training job that can be monitored in the SageMaker console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-23 18:43:43 Starting - Starting the training job......\n",
      "2020-04-23 18:44:13 Starting - Launching requested ML instances...\n",
      "2020-04-23 18:45:11 Starting - Preparing the instances for training......\n",
      "2020-04-23 18:45:55 Downloading - Downloading input data...\n",
      "2020-04-23 18:46:42 Training - Training image download completed. Training in progress..\u001b[34m2020-04-23 18:46:42,901 sagemaker-containers INFO     Imported framework sagemaker_sklearn_container.training\u001b[0m\n",
      "\u001b[34m2020-04-23 18:46:42,903 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-04-23 18:46:42,913 sagemaker_sklearn_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-04-23 18:46:43,201 sagemaker-containers INFO     Module train does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-04-23 18:46:43,201 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-04-23 18:46:43,201 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-04-23 18:46:43,201 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python -m pip install . \u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: train\n",
      "  Building wheel for train (setup.py): started\n",
      "  Building wheel for train (setup.py): finished with status 'done'\n",
      "  Created wheel for train: filename=train-1.0.0-py2.py3-none-any.whl size=7564 sha256=093ab1c6993ade171bbeb83872d255af451cfa8a20ab1583d0eed893f4e1780b\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-rfd_ttge/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[34mSuccessfully built train\u001b[0m\n",
      "\u001b[34mInstalling collected packages: train\u001b[0m\n",
      "\u001b[34mSuccessfully installed train-1.0.0\u001b[0m\n",
      "\u001b[34m2020-04-23 18:46:44,561 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-04-23 18:46:44,571 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_sklearn_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-scikit-learn-2020-04-23-18-43-42-887\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-2-875690977746/sagemaker-scikit-learn-2020-04-23-18-43-42-887/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_sklearn_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-2-875690977746/sagemaker-scikit-learn-2020-04-23-18-43-42-887/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_sklearn_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-scikit-learn-2020-04-23-18-43-42-887\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-875690977746/sagemaker-scikit-learn-2020-04-23-18-43-42-887/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/miniconda3/bin:/miniconda3/lib/python37.zip:/miniconda3/lib/python3.7:/miniconda3/lib/python3.7/lib-dynload:/miniconda3/lib/python3.7/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python -m train\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[34m2020-04-23 18:46:45,937 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-04-23 18:46:54 Uploading - Uploading generated training model\n",
      "2020-04-23 18:46:54 Completed - Training job completed\n",
      "Training seconds: 59\n",
      "Billable seconds: 59\n",
      "CPU times: user 448 ms, sys: 29.6 ms, total: 478 ms\n",
      "Wall time: 3min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "test_path = f's3://{bucket}/{data_dir}/{prefix}/test.csv'\n",
    "train_path = f's3://{bucket}/{data_dir}/{prefix}/train.csv'\n",
    "\n",
    "data_channels = {\n",
    "    \"train\": train_path,\n",
    "    \"test\": test_path\n",
    "}\n",
    "\n",
    "# Train estimator on S3 training data\n",
    "gbm_estimator.fit(inputs=data_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the trained model\n",
    "\n",
    "After training, we can deploy the model to create a `predictor`.\n",
    "\n",
    "To deploy a trained model, we to use `<model>.deploy`, which takes in two arguments:\n",
    "* **initial_instance_count**: The number of deployed instances (1).\n",
    "* **instance_type**: The type of SageMaker instance for deployment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------!CPU times: user 276 ms, sys: 18.6 ms, total: 295 ms\n",
      "Wall time: 8min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# deploy the model to create a predictor\n",
    "predictor = gbm_estimator.deploy(initial_instance_count=1,\n",
    "                                instance_type='ml.t2.medium')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Evaluating The Model\n",
    "\n",
    "Once the model is deployed, we can see how it performs when applied to the test data. We have already trained this model with the training set only. This is the long awaited hold-out set. We could have passed the test data to the model already, but I wanted to pass it to the deployed model as proof that it is actually deployed and as a fun hold-out for the real goal of this project -- using SageMaker. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plagiarism_project\n"
     ]
    }
   ],
   "source": [
    "print(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# read in test data, assuming it is stored locally\n",
    "test_data = pd.read_csv(os.path.join('plagiarism_data', \"test.csv\"), header=None, names=None)\n",
    "\n",
    "# labels are in the first column\n",
    "test_y = test_data.iloc[:,0]\n",
    "test_x = test_data.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy of the model\n",
    "\n",
    "We will use the deployed `predictor` to generate predicted, class labels for the test data. Comparing those to the *true* labels, `test_y`, and calculate the accuracy that the model classified correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "test_y_preds = predictor.predict(test_x)\n",
    "\n",
    "# Test\n",
    "assert len(test_y_preds)==len(test_y), 'Unexpected number of predictions.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.92 %\n",
      "\n",
      "Predicted class labels: \n",
      "[0 0 0 0 0 1 1 0 0 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 0]\n",
      "\n",
      "True class labels: \n",
      "[0 0 0 0 1 1 1 0 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Accuracy score\n",
    "accuracy = accuracy_score(y_true=test_y, y_pred=test_y_preds)\n",
    "cls_report = classification_report(test_y, test_y_preds)\n",
    "confusion_mat = confusion_matrix(test_y, test_y_preds)\n",
    "print('accuracy:', accuracy, '%')\n",
    "\n",
    "\n",
    "# Print out the array of predicted and true labels\n",
    "print('\\nPredicted class labels: ')\n",
    "print(test_y_preds)\n",
    "print('\\nTrue class labels: ')\n",
    "print(test_y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      1.00      0.91        10\n",
      "           1       1.00      0.87      0.93        15\n",
      "\n",
      "   micro avg       0.92      0.92      0.92        25\n",
      "   macro avg       0.92      0.93      0.92        25\n",
      "weighted avg       0.93      0.92      0.92        25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(cls_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10  0]\n",
      " [ 2 13]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two wrongly identified cases on the test set, not bad. Both were cases of False Negatives, which seems like the right side to be on. I would rather a model miss a case than a model that flags authentic work as phony. As expected the test data proved to be a bit harder for the model to predict than the training set evaluation scored. This is generally true for train/test data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Clean up Resources\n",
    "\n",
    "These are for-pay services from AWS so always be sure to delete endpooints once finished so as not to incure additionally costs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
